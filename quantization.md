# 인공지능의 양자화에 대해 다룹니다. 양자역학 아님

## 양자화란?

[0, a]에서 적분 가능한 함수 f(x)가 있다고 하자. (여기서 a는 0보다 큰 실수)

이 함수를 정적분 한다고 하면, 범위 내에 사각형을 채워넣는걸 생각할 수 있다.

<img width="1051" height="542" alt="image" src="https://github.com/user-attachments/assets/112063f1-66aa-462f-91a6-dbcbd4b92926" />

사각형의 폭이 좁아지고, 갯수가 점점 많아지면 함수의 면적 = 적분값이 된다.

```

Σ f(x_i)Δx = F(x)

```

여기서 F(x)는 f(x)을 적분한 함수.


**양자화는 이 과정의 역산이라고 볼 수 있다.**

### 양자역학의 파동함수

일반적으로 파동함수는 다음과 같이 표현한다.(여기선, 간단히 표현하기 위해 time independent 파동함수를 썼고, einstein notation을 사용하였다.)

```

Ψ(x) = ψ_i(x)

```

이 함수의 형태는 위에서 말한 정적분의 합기호 형태의 식과 같다.

양자역학에서는 모든 상태가 아닌 특정 상태만 가능하며, 이로 인해 특정 에너지 값만 가질 수 있다.

### 그래서 인공지능의 양자화랑 어떤 관계인가??

앞서 말한 것처럼, 양자역학의 핵심 중 하나는 다음과 같다.

**모든 상태가 아닌 특정 상태만 가능하며, 이로 인해 특정 에너지 값만 가질 수 있다.**

인공지능 학습 중 **가중치, 편향, 활성화 값**이 양자화 된다.

예를 들어서 일반적인 경우에 가중치는 모든 값을 가질 수가 있지만, 양자화를 한다면 0.1, 0.2, 0.3... 0.1단위로 뛰는 값밖에 가질 수 없다.

### 양자화의 장점??

양자화의 큰 장점은 데이터 압축에 있다.

예를 들어 1~10까지의 값을 전부 나열한다고 한다면, 실수 범위에서는 무한대가 존재하지만, 정수 범위에서는 총 10개만 존재할 수 있다.

이러한 원리를 이용하여 가중치, 편향, 활성화값이 제한된다면 예측값도 제한될 수 있고, 결과적으로 데이터가 줄어든다.

예를 들어 32비트 가중치를 8비트로 양자화한다면 4바이트에서 1바이트로 줄어듦. --> 메모리 사용량 1/4로 감소.

연산 속도는 정비례하는 건 아니지만, 데이터크기 감소, 연산량 감소로 인해 연산 속도도 증가함.

FPGA, ASIC 등 하드웨어 가속기에서 매우 유리함

### 단점?

앞서 보여준 그래프에서 보면 함수의 면적과 사각형의 면적의 갭이 존재한다. 사각형의 폭이 좁아지고, 조밀해질수록 면적의 갭은 줄어들지만,

양자화는 정적분의 역산이므로 갭이 존재할 수 밖에 없다.

이로 인해 손실이 발생할 수 밖에 없다. --> 구조적 문제이므로 완전 극복은 불가능.

비트를 많이 줄일 수록 손실이 크게 날 수 밖에 없음.

### 세 줄 요약

**양자화는 일종의 손실 압축**

**메모리 감소로 인해 FPGA, ASIC같은 하드웨어 가속기에서 효율적**

**구조적으로 손실이 발생할 수 밖에 없음.**



